# End-to-End ELT Pipeline with dbt, Snowflake & Airflow

This project demonstrates an end-to-end **ELT pipeline** using **dbt**, **Snowflake**, and **Apache Airflow**, orchestrated in a containerized environment via the **Astro CLI**. It showcases a modern, testable, and production-aligned data engineering workflowâ€”from raw data ingestion to final reporting layers.

---

## ğŸš€ Overview

**Pipeline Flow:**

1. Extract sample data from **Snowflake Sample TPCH Dataset**
2. Transform it using **dbt models**, **macros**, and **modular SQL layers**
3. Apply **automated tests** to validate transformations
4. Orchestrate and schedule runs with **Apache Airflow DAGs**
5. Run locally using **Astro CLI (Dockerized Airflow)**

---

## âš™ï¸ Tools & Technologies

| Tool                | Purpose                                        |
|---------------------|------------------------------------------------|
| **Snowflake**       | Cloud data warehouse (source + target)         |
| **dbt**             | SQL transformations, testing, and lineage      |
| **Apache Airflow**  | Workflow orchestration via DAGs                |
| **Astronomer Cosmos** | Connects dbt projects to Airflow             |
| **Docker / Astro CLI** | Local dev environment for airflow & dbt     |
| **Python**          | DAG and orchestration logic                    |

---


## ğŸ“ Project Structure

```bash
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ dbt/
â”‚       â””â”€â”€ data_pipeline/
â”‚           â”œâ”€â”€ models/
â”‚           â”‚   â”œâ”€â”€ staging/
â”‚           â”‚   â””â”€â”€ marts/
â”‚           â”œâ”€â”€ macros/
â”‚           â”œâ”€â”€ seeds/
â”‚           â”œâ”€â”€ snapshots/
â”‚           â”œâ”€â”€ tests/
â”‚           â”œâ”€â”€ dbt_project.yml
â”‚           â””â”€â”€ README.md
â”‚   â”œâ”€â”€ dbt_dag.py
â”‚   â””â”€â”€ .airflowignore
â”œâ”€â”€ include/
â”œâ”€â”€ plugins/
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dags/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .astro/
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ airflow_settings.yaml
â””â”€â”€ README.md



---


âœ… Key Features
ğŸ§± Modular dbt model design (staging â†’ intermediate â†’ fact layers)

ğŸ” Custom macros for DRY SQL logic (e.g., discount calculations)

ğŸ§ª Integrated data quality testing:

Generic tests: unique, not_null, relationships, accepted_values

Singular tests: invalid discounts, invalid dates

âš™ï¸ Full DAG orchestration via Cosmos' DbtDag in Airflow

ğŸ³ Dockerized development using Astro CLI for consistent local environment

ğŸ§ª DAG: Orchestration Flow
text
Copy
Edit
stg_tpch_orders â†’ stg_tpch_line_items_run
                          â†“
                int_order_items_run
                          â†“
         int_order_items_summary_run
                          â†“
                   fct_orders
Each step corresponds to a dbt model or test and is handled by Cosmos' DbtDag integration inside Airflow.

ğŸ§¬ Data Validation Tests
Test Type	Description
Generic	Uniqueness, Not Null, Relationships, Accepted Values
Singular	Custom SQL to check date ranges and discount logic
ğŸ¯ Why This Project Matters
This project simulates how modern data teams build real pipelines. It reflects critical skills for Data Engineering, Analytics Engineering, and Cloud ETL roles:

âœ… Real-world use of the modern data stack

ğŸ§± Layered dbt model design with ref-based dependencies

âš™ï¸ Proficiency in workflow orchestration using DAGs

ğŸ§ª Implementation of data quality tests across layers

ğŸ³ Experience with containerized, reproducible environments

The pipeline is modular, testable, reproducible, and ready for scaling or adaptation to production platforms.

ğŸ“¸ DAG Execution Snapshot

ğŸ”— Repository
GitHub: End-to-End-ELT-Pipeline-with-dbt-Snowflake-Airflow

python
Copy
Edit

### âœ… Next Steps (Optional)
Let me know if you'd like me to:
- Add a small "How to Run Locally" section
- Rename `README_assets` and move the DAG image accordingly
- Convert this into a **Notion or Portfolio writeup**

