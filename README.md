# End-to-End ELT Pipeline with dbt, Snowflake & Airflow

This project demonstrates an end-to-end **ELT pipeline** using **dbt**, **Snowflake**, and **Apache Airflow**, orchestrated in a containerized environment via the **Astro CLI**. It reflects a modern data engineering approachâ€”modular, testable, and aligned with production standards.

---

## ğŸš€ Overview

This project simulates a real-world data transformation workflow using the modern data stack. Here's what it covers:

1. Extract sample data from the **Snowflake TPCH dataset**
2. Transform and model it using **dbt** with **modular SQL layers**
3. Apply **data quality testing** (generic and custom)
4. Schedule and orchestrate runs using **Apache Airflow DAGs**
5. Run locally inside containers using **Astro CLI**

---

## âš™ï¸ Tools & Technologies

| Tool                  | Purpose                                        |
|-----------------------|------------------------------------------------|
| **Snowflake**         | Cloud data warehouse (source & destination)    |
| **dbt**               | SQL-based modeling, macros, and testing        |
| **Apache Airflow**    | Orchestration engine for scheduling workflows  |
| **Astronomer Cosmos** | Seamless integration between dbt & Airflow     |
| **Docker / Astro CLI**| Containerized local dev environment            |
| **Python**            | DAG construction and operator logic            |

---

## ğŸ“ Project Structure

```bash
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ dbt/
â”‚       â””â”€â”€ data_pipeline/
â”‚           â”œâ”€â”€ models/
â”‚           â”‚   â”œâ”€â”€ staging/
â”‚           â”‚   â””â”€â”€ marts/
â”‚           â”œâ”€â”€ macros/
â”‚           â”œâ”€â”€ seeds/
â”‚           â”œâ”€â”€ snapshots/
â”‚           â”œâ”€â”€ tests/
â”‚           â”œâ”€â”€ dbt_project.yml
â”‚           â””â”€â”€ README.md
â”‚   â”œâ”€â”€ dbt_dag.py
â”‚   â””â”€â”€ .airflowignore
â”œâ”€â”€ include/
â”œâ”€â”€ plugins/
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dags/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .astro/
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ airflow_settings.yaml
â””â”€â”€ README.md

```  

---


âœ… Key Features
ğŸ§± Modular dbt model design (staging â†’ intermediate â†’ fact layers)

ğŸ” Custom macros for DRY SQL logic (e.g., discount calculations)

ğŸ§ª Integrated data quality testing:

Generic tests: unique, not_null, relationships, accepted_values

Singular tests: invalid discounts, invalid dates

âš™ï¸ Full DAG orchestration via Cosmos' DbtDag in Airflow

ğŸ³ Dockerized development using Astro CLI for consistent local environment

ğŸ§ª DAG: Orchestration Flow
text
Copy
Edit
stg_tpch_orders â†’ stg_tpch_line_items_run
                          â†“
                int_order_items_run
                          â†“
         int_order_items_summary_run
                          â†“
                   fct_orders
Each step corresponds to a dbt model or test and is handled by Cosmos' DbtDag integration inside Airflow.

ğŸ§¬ Data Validation Tests
Test Type	Description
Generic	Uniqueness, Not Null, Relationships, Accepted Values
Singular	Custom SQL to check date ranges and discount logic
ğŸ¯ Why This Project Matters
This project simulates how modern data teams build real pipelines. It reflects critical skills for Data Engineering, Analytics Engineering, and Cloud ETL roles:

âœ… Real-world use of the modern data stack

ğŸ§± Layered dbt model design with ref-based dependencies

âš™ï¸ Proficiency in workflow orchestration using DAGs

ğŸ§ª Implementation of data quality tests across layers

ğŸ³ Experience with containerized, reproducible environments

The pipeline is modular, testable, reproducible, and ready for scaling or adaptation to production platforms.

ğŸ“¸ DAG Execution Snapshot


