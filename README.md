# End-to-End ELT Pipeline with dbt, Snowflake & Airflow

This project demonstrates an end-to-end **ELT pipeline** using **dbt**, **Snowflake**, and **Apache Airflow**â€”orchestrated and containerized with the **Astro CLI**. It reflects a modern data engineering workflow: raw data ingestion, transformation, testing, and scheduling through automated DAGs.

---

## ğŸš€ Overview

The goal of this project is to showcase how to:

- Extract sample data from **Snowflake Sample TPCH Dataset**
- Transform it using **dbt models and macros**
- Test and validate outputs using **dbt tests**
- Schedule the DAG with **Apache Airflow**
- Run locally with **Astro CLI** (Dockerized environment)

This is a real-world, modular pipeline that is scalable, testable, and production-ready.

---

## âš™ï¸ Tools & Technologies

| Tool                | Purpose                             |
|---------------------|-------------------------------------|
| **Snowflake**       | Cloud data warehouse (data source + target) |
| **dbt**             | SQL-based transformation & testing |
| **Astronomer Cosmos** | Integration between dbt & Airflow |
| **Apache Airflow**  | Workflow orchestration |
| **Docker / Astro CLI** | Local containerized environment |
| **Python**          | DAG definition and orchestration |

---

## ğŸ“ Project Structure

```bash
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ dbt/
â”‚       â””â”€â”€ data_pipeline/
â”‚           â”œâ”€â”€ models/
â”‚           â”‚   â”œâ”€â”€ staging/
â”‚           â”‚   â””â”€â”€ marts/
â”‚           â”œâ”€â”€ macros/
â”‚           â”œâ”€â”€ seeds/
â”‚           â”œâ”€â”€ snapshots/
â”‚           â”œâ”€â”€ tests/
â”‚           â”œâ”€â”€ dbt_project.yml
â”‚           â””â”€â”€ README.md
â”‚   â”œâ”€â”€ dbt_dag.py
â”‚   â””â”€â”€ .airflowignore
â”œâ”€â”€ include/
â”œâ”€â”€ plugins/
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dags/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .astro/
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ airflow_settings.yaml
â””â”€â”€ README.md
